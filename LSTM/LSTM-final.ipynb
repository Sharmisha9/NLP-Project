{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS521 Final Project - Next Words Prediction in Joke \n",
    "### Sharmisha Parvathaneni \n",
    "\n",
    "##### This notebook details the development and validation of a Long Short-Term Memory (LSTM) model aimed at predicting next words in Joke. It includes data preprocessing, model training, evaluation, and final predictions.\n",
    "\n",
    "\n",
    "#####  - Dataset URL: [Short Jokes Dataset](https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes)\n",
    "#####  - Number of Records: 100000\n",
    "##### - Number of Fields: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 03:14:50.758905: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 03:14:50.763746: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 03:14:50.828517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Telling my daughter garlic is good for you. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I've been going through a really rough period ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If I could have dinner with anyone, dead or al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Two guys walk into a bar. The third guy ducks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  [me narrating a documentary about narrators] \"...\n",
       "1   2  Telling my daughter garlic is good for you. Go...\n",
       "2   3  I've been going through a really rough period ...\n",
       "3   4  If I could have dinner with anyone, dead or al...\n",
       "4   5     Two guys walk into a bar. The third guy ducks."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "jokes_df = pd.read_csv('shortjokes.csv')\n",
    "jokes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  100000\n",
      "Number of fields:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records: \", jokes_df.shape[0])\n",
    "print(\"Number of fields: \", jokes_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [me narrating a documentary about narrators] \"...\n",
       "1        Telling my daughter garlic is good for you. Go...\n",
       "2        I've been going through a really rough period ...\n",
       "3        If I could have dinner with anyone, dead or al...\n",
       "4           Two guys walk into a bar. The third guy ducks.\n",
       "                               ...                        \n",
       "99995    Every time I walk into a singles bar I can hea...\n",
       "99996    how wide is the universe? how long is a piece ...\n",
       "99997    A man goes to a halloween party wearing nothin...\n",
       "99998                           I don't Bolivia Peru-v it.\n",
       "99999    What's the world's longest Ted Talk? How I Met...\n",
       "Name: Joke, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_df['Joke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>Cleaned_Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "      <td>me narrating documentary about narrators can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telling my daughter garlic is good for you. Go...</td>\n",
       "      <td>telling my daughter garlic is good for you goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've been going through a really rough period ...</td>\n",
       "      <td>i ve been going through really rough period at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I could have dinner with anyone, dead or al...</td>\n",
       "      <td>if could have dinner with anyone dead or alive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two guys walk into a bar. The third guy ducks.</td>\n",
       "      <td>two guys walk into bar the third guy ducks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Every time I walk into a singles bar I can hea...</td>\n",
       "      <td>every time walk into singles bar can hear mom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>how wide is the universe? how long is a piece ...</td>\n",
       "      <td>how wide is the universe how long is piece of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>A man goes to a halloween party wearing nothin...</td>\n",
       "      <td>a man goes to halloween party wearing nothing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>I don't Bolivia Peru-v it.</td>\n",
       "      <td>i don bolivia peru it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>What's the world's longest Ted Talk? How I Met...</td>\n",
       "      <td>what the world longest ted talk how met your m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Joke  \\\n",
       "0      [me narrating a documentary about narrators] \"...   \n",
       "1      Telling my daughter garlic is good for you. Go...   \n",
       "2      I've been going through a really rough period ...   \n",
       "3      If I could have dinner with anyone, dead or al...   \n",
       "4         Two guys walk into a bar. The third guy ducks.   \n",
       "...                                                  ...   \n",
       "99995  Every time I walk into a singles bar I can hea...   \n",
       "99996  how wide is the universe? how long is a piece ...   \n",
       "99997  A man goes to a halloween party wearing nothin...   \n",
       "99998                         I don't Bolivia Peru-v it.   \n",
       "99999  What's the world's longest Ted Talk? How I Met...   \n",
       "\n",
       "                                            Cleaned_Joke  \n",
       "0       me narrating documentary about narrators can ...  \n",
       "1      telling my daughter garlic is good for you goo...  \n",
       "2      i ve been going through really rough period at...  \n",
       "3      if could have dinner with anyone dead or alive...  \n",
       "4            two guys walk into bar the third guy ducks   \n",
       "...                                                  ...  \n",
       "99995  every time walk into singles bar can hear mom ...  \n",
       "99996  how wide is the universe how long is piece of ...  \n",
       "99997  a man goes to halloween party wearing nothing ...  \n",
       "99998                             i don bolivia peru it   \n",
       "99999  what the world longest ted talk how met your m...  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "\n",
    "# Convert to lowercase\n",
    "    text = text.lower()\n",
    "# Removing non-words\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "#Removing Single Characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "#Removing Extra Spaces\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to the Joke column\n",
    "jokes_df['Cleaned_Joke'] = jokes_df['Joke'].apply(clean_text)\n",
    "\n",
    "jokes_df[['Joke', 'Cleaned_Joke']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "![Alt Text](../images/Tokenization.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizer process\n",
    "tokenizer = Tokenizer()  # Creates an instance of the Tokenizer class from the Keras preprocessing library.\n",
    "#fit\n",
    "tokenizer.fit_on_texts(jokes_df['Joke']) # Processes the text data to extract unique words and assign an index to each word.\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By creating a vocabulary index (word_index), the tokenizer provides a consistent way to convert back and forth between words and their corresponding numeric indices. This consistency is important for both training the model and using it to generate new text.\n",
    "\n",
    "#####  In the context of predicting the next word in a joke, the model needs to understand the sequence of words that come before to make a prediction about what comes next. Each word in a sequence is input as a numeric index, and the model learns to predict the next index (word) in the sequence. During inference, these predictions are converted back to words using the tokenizer's vocabulary. With tokenization, models can effectively capture the relationships between words in sequences, learning patterns like syntax, semantics, and common phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating n-grams from Tokenized Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#declaring ngrams\n",
    "input_sequences = []\n",
    "for line in jokes_df['Joke']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    # Generating n-grams from Token Sequences\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By training on n-gram sequences, LSTMs learn the context gradually. For instance, in a training set consisting of the sentence \"The quick brown fox\", the LSTM would see sequences like [\"The\", \"quick\"], [\"The\", \"quick\", \"brown\"], and use each to predict the next word. This step-by-step buildup of sequences helps the model understand how the presence of certain words affects the likelihood of following words, a concept crucial for generating coherent text.\n",
    "##### Generating multiple subsequences from each joke maximizes the utilization of the data, creating numerous training examples from a single line of text. This approach improves the model's exposure to different patterns and contexts within the data, enhancing its ability to generalize and predict accurately in varied situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 8701],\n",
       " [14, 8701, 1],\n",
       " [14, 8701, 1, 3039],\n",
       " [14, 8701, 1, 3039, 43],\n",
       " [14, 8701, 1, 3039, 43, 25110]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculates the maximum length of sequences within the list input_sequences\n",
    "max_len = max([len(x) for x in input_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# This function transforms a list (input_sequences) of numeric sequences (lists of integers where each integer represents a word token) into a 2D numpy array of shape.\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,   14, 8701],\n",
       "       [   0,    0,    0, ...,   14, 8701,    1],\n",
       "       [   0,    0,    0, ..., 8701,    1, 3039],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   27,    3,  620],\n",
       "       [   0,    0,    0, ...,    3,  620,   23],\n",
       "       [   0,    0,    0, ...,  620,   23,  379]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural networks require inputs to be of consistent size and shape. The pad_sequences function standardizes the length of all input sequences, which is essential for training these models efficiently.\n",
    "\n",
    "##### By padding sequences, particularly with pre-padding, the model focuses more on the most recent inputs (the actual words) when making predictions. This is especially important in tasks like predicting the next word where the context closer to the word being predicted is usually more relevant than the earlier context.\n",
    "\n",
    "##### Many types of neural networks, such as LSTMs (Long Short-Term Memory networks) or GRUs (Gated Recurrent Units), are designed to work with sequence data and often expect all inputs to be of uniform length. Padding ensures compatibility with these architectural requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines the input features for a model by selecting all rows of padded_input_sequences and all columns \n",
    "X = padded_input_sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracts the target variable\n",
    "y = padded_input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Processing batches of data, specifically for converting sequences of integers (representing words) into one-hot encoded formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_rows = 100000\n",
    "total_words = 46923\n",
    "batch_size = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generator Function (batch_generator)\n",
    "def batch_generator(y, batch_size, total_words):\n",
    "    num_batches = int(np.ceil(len(y) / batch_size))\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(y))  \n",
    "        y_batch = y[start:end]\n",
    "        y_batch_encoded = tf.keras.utils.to_categorical(y_batch, num_classes=total_words) # Converts the numeric labels in y_batch into a one-hot encoded format\n",
    "        yield y_batch_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = padded_input_sequences[:,-1]\n",
    "# Using the generator to process data\n",
    "for encoded_batch in batch_generator(y, batch_size, total_words):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a generator is memory-efficient, especially important when dealing with large datasets (100,000 rows). It prevents loading all data into memory at once, which could lead to high memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Sequential model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,692,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46923</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,085,373</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │     \u001b[38;5;34m4,692,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m150,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46923\u001b[0m)          │     \u001b[38;5;34m7,085,373\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,928,273</span> (45.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,928,273\u001b[0m (45.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,928,273</span> (45.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,928,273\u001b[0m (45.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "total_words = 46923\n",
    "  \n",
    "max_sequence_len = 10  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Adding Layers to the Model\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By training this model on sequences derived from jokes, it learns the patterns in how words are sequenced to form jokes, enabling it to generate or continue jokes by predicting subsequent words.\n",
    "\n",
    "\n",
    "##### Model Summary\n",
    "##### Layer Information:\n",
    "##### Embedding Layer: Converts integer-encoded words to dense vectors of a fixed size (100 dimensions). It shows that the model has 46,923 possible words (vocab size) and outputs a sequence length of 9 (input length) for each batch.\n",
    "##### Output Shape: (None, 9, 100) indicates the batch size (None is any batch size), sequence length (9), and embedding dimension (100).\n",
    "##### Parameters: 4,692,300 parameters, calculated as the product of the number of words in the vocabulary and the embedding dimensions (total_words * embedding_dimension).\n",
    "##### LSTM Layer: A type of recurrent neural network that can learn order dependence in sequence prediction problems.\n",
    "##### Output Shape: (None, 150) shows that for each sequence in the batch, the LSTM outputs a single vector of 150 features that captures the information from the sequence.\n",
    "##### Parameters: 150,600 parameters, which are derived from the internal calculations of the LSTM (involving several gates and states).\n",
    "##### Dense Layer: This is the output layer of the model, using a softmax activation function. It maps the output of the LSTM into a probability distribution over the 46,923 possible next words.\n",
    "##### Output Shape: (None, 46923) indicates that for each input sequence, the model predicts a probability distribution across all 46,923 words.\n",
    "##### Parameters: 7,085,373, which comes from multiplying the output size of the LSTM by the number of words in the vocabulary, plus bias terms for each word.\n",
    "\n",
    "##### Total Parameters:\n",
    "##### 11,928,273 Total Parameters (45.50 MB): This is the sum of parameters from all layers. The memory size (45.50 MB) indicates the space required to store these parameters in memory. All parameters are trainable, meaning they are updated during the training process.\n",
    "\n",
    "\n",
    "##### The large number of parameters (almost 12 million) signifies a highly flexible model that can learn complex patterns in data. The extensive embedding and output layers allow the model to handle a large vocabulary with nuanced understanding. The LSTM's ability to remember and utilize context effectively makes it ideal for tasks like text generation where the meaning depends heavily on preceding words. This is critical in humor, where context can drastically alter the meaning.\n",
    "\n",
    "##### By using a softmax activation in the output layer, the model is trained to output a probability distribution over all possible words. The word with the highest probability is selected as the predicted next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1411s\u001b[0m 435ms/step - accuracy: 0.0721 - loss: 6.9527\n",
      "Epoch 2/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1478s\u001b[0m 440ms/step - accuracy: 0.1607 - loss: 5.7324\n",
      "Epoch 3/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1387s\u001b[0m 428ms/step - accuracy: 0.2303 - loss: 4.6811\n",
      "Epoch 7/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 425ms/step - accuracy: 0.2417 - loss: 4.5312\n",
      "Epoch 8/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 423ms/step - accuracy: 0.2528 - loss: 4.4042\n",
      "Epoch 9/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1410s\u001b[0m 435ms/step - accuracy: 0.2635 - loss: 4.2874\n",
      "Epoch 10/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1438s\u001b[0m 427ms/step - accuracy: 0.2742 - loss: 4.1835\n",
      "Epoch 11/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1380s\u001b[0m 426ms/step - accuracy: 0.2836 - loss: 4.0947\n",
      "Epoch 12/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1371s\u001b[0m 416ms/step - accuracy: 0.2932 - loss: 4.0127\n",
      "Epoch 13/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1401s\u001b[0m 416ms/step - accuracy: 0.3019 - loss: 3.9382\n",
      "Epoch 14/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1318s\u001b[0m 406ms/step - accuracy: 0.3104 - loss: 3.8739\n",
      "Epoch 15/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1351s\u001b[0m 417ms/step - accuracy: 0.3175 - loss: 3.8168\n",
      "Epoch 16/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1475s\u001b[0m 439ms/step - accuracy: 0.3250 - loss: 3.7562\n",
      "Epoch 17/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1315s\u001b[0m 406ms/step - accuracy: 0.3323 - loss: 3.7024\n",
      "Epoch 18/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1299s\u001b[0m 401ms/step - accuracy: 0.3380 - loss: 3.6554\n",
      "Epoch 19/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1323s\u001b[0m 408ms/step - accuracy: 0.3438 - loss: 3.6130\n",
      "Epoch 20/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1307s\u001b[0m 397ms/step - accuracy: 0.3491 - loss: 3.5734\n",
      "Epoch 21/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1378s\u001b[0m 408ms/step - accuracy: 0.3545 - loss: 3.5338\n",
      "Epoch 22/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1322s\u001b[0m 408ms/step - accuracy: 0.3581 - loss: 3.5007\n",
      "Epoch 23/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1313s\u001b[0m 399ms/step - accuracy: 0.3627 - loss: 3.4711\n",
      "Epoch 24/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1367s\u001b[0m 406ms/step - accuracy: 0.3672 - loss: 3.4377\n",
      "Epoch 25/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1321s\u001b[0m 407ms/step - accuracy: 0.3715 - loss: 3.4118\n",
      "Epoch 26/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1327s\u001b[0m 403ms/step - accuracy: 0.3750 - loss: 3.3825\n",
      "Epoch 27/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1307s\u001b[0m 403ms/step - accuracy: 0.3778 - loss: 3.3605\n",
      "Epoch 28/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1348s\u001b[0m 405ms/step - accuracy: 0.3820 - loss: 3.3328\n",
      "Epoch 29/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1346s\u001b[0m 406ms/step - accuracy: 0.3839 - loss: 3.3126\n",
      "Epoch 30/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1334s\u001b[0m 404ms/step - accuracy: 0.3867 - loss: 3.2957\n",
      "Epoch 31/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1312s\u001b[0m 405ms/step - accuracy: 0.3901 - loss: 3.2741\n",
      "Epoch 32/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1337s\u001b[0m 403ms/step - accuracy: 0.3921 - loss: 3.2560\n",
      "Epoch 33/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1396s\u001b[0m 428ms/step - accuracy: 0.3976 - loss: 3.2204\n",
      "Epoch 35/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1403s\u001b[0m 428ms/step - accuracy: 0.3999 - loss: 3.2049\n",
      "Epoch 36/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1252s\u001b[0m 386ms/step - accuracy: 0.4018 - loss: 3.1869\n",
      "Epoch 37/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1336s\u001b[0m 403ms/step - accuracy: 0.4042 - loss: 3.1758\n",
      "Epoch 38/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1287s\u001b[0m 397ms/step - accuracy: 0.4076 - loss: 3.1506\n",
      "Epoch 40/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1310s\u001b[0m 404ms/step - accuracy: 0.4098 - loss: 3.1343\n",
      "Epoch 41/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1351s\u001b[0m 407ms/step - accuracy: 0.4112 - loss: 3.1241\n",
      "Epoch 42/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 410ms/step - accuracy: 0.4130 - loss: 3.1157\n",
      "Epoch 43/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1329s\u001b[0m 406ms/step - accuracy: 0.4147 - loss: 3.1013\n",
      "Epoch 44/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1300s\u001b[0m 393ms/step - accuracy: 0.4166 - loss: 3.0881\n",
      "Epoch 45/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1304s\u001b[0m 400ms/step - accuracy: 0.4177 - loss: 3.0814\n",
      "Epoch 46/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1366s\u001b[0m 407ms/step - accuracy: 0.4181 - loss: 3.0786\n",
      "Epoch 47/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1336s\u001b[0m 406ms/step - accuracy: 0.4200 - loss: 3.0614\n",
      "Epoch 48/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1344s\u001b[0m 406ms/step - accuracy: 0.4207 - loss: 3.0555\n",
      "Epoch 49/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1317s\u001b[0m 406ms/step - accuracy: 0.4221 - loss: 3.0485\n",
      "Epoch 50/50\n",
      "\u001b[1m3243/3243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1333s\u001b[0m 403ms/step - accuracy: 0.4228 - loss: 3.0415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f24827471c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Convert labels to a sparse matrix format\n",
    "y_sparse = np.array(y, dtype='int32')  \n",
    "\n",
    "# sparse categorical crossentropy loss to handle sparse labels\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y_sparse, epochs=50,  batch_size= 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From above accuracy and loss, we can understand that increasing epochs to 100 or 150, adjusting the learning rate, experimenting with different batch size and choosing a diverse dataset could have increased the accuracy. \n",
    "\n",
    "##### The LSTM model was able to predict the next words with a moderate degree of accuracy, as evidenced by the final accuracy level of approximately 42%. This suggests that while the LSTM can capture some patterns and structures inherent to the dataset, it struggles to produce coherent and contextually appropriate continuations that form a complete, well-structured joke. \n",
    "\n",
    "##### The LSTM's performance suggests that while it has learned some patterns in the data, humor generation requires more sophisticated understanding and modeling of language nuances, cultural references, and comedic timing, which are areas where GPTs excel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To improve upon the LSTM model's capabilities, one might consider \n",
    "##### Implementing and experimenting with attention mechanisms within LSTM models could potentially improve performance by enabling the model to focus on critical parts of the input sequence.\n",
    "##### Experiment with more sophisticated pre-training strategies that specifically focus on humor. This could involve curating a large dataset with varied comedic styles and using it to pre-train an LSTM before fine-tuning on joke-specific data.\n",
    "##### Apply meta-learning techniques so that the model can quickly adapt to new styles of humor or content with minimal additional data, enabling faster personalization and responsiveness to trends.\n",
    "##### Hybrid models or a switch to transformer-based architectures like GPT, which have been shown to perform better in text generation tasks due to their attention mechanisms and pre-training on vast datasets.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
